{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast, random, pickle, pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Random Baseline\n",
    "\n",
    "We don't want the random baseline to be entirely random but to reflect the distribution of labels found in the corpus. Therefore, we will use the given label distributions to generate random label predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train + test data\n",
    "train_cleaned = pd.read_csv('data/dpm_train.csv')\n",
    "test = pd.read_csv('data/dpm_test.csv')\n",
    "\n",
    "# load train labels (lower level & higher level)\n",
    "# we need to turn the labels (currently list of strings) into a list of lists\n",
    "y_train_low = train_cleaned.loc[:, 'label'].to_numpy()\n",
    "y_train_low = np.array([np.array(ast.literal_eval(x)) for x in y_train_low])\n",
    "y_train_high = train_cleaned.loc[:, 'higher level label'].to_numpy()\n",
    "y_train_high = np.array([np.array(ast.literal_eval(x)) for x in y_train_high])\n",
    "\n",
    "# load test labels (lower level & higher level)\n",
    "# we need to turn the labels (currently list of strings) into a list of lists\n",
    "y_test_low = test.loc[:, 'label'].to_numpy()\n",
    "y_test_low = np.array([np.array(ast.literal_eval(x)) for x in y_test_low])\n",
    "y_test_high = test.loc[:, 'higher level label'].to_numpy()\n",
    "y_test_high = np.array([np.array(ast.literal_eval(x)) for x in y_test_high])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of lower level labels: [0.70184049 0.19263804 0.2196319  0.22453988 0.19386503 0.45889571\n",
      " 0.03680982]\n",
      "Distribution of higher level labels: [0.71288344 0.36809816 0.54601227]\n"
     ]
    }
   ],
   "source": [
    "# find out label distributions\n",
    "dist_low = y_train_low.sum(axis=0)/len(train_cleaned)\n",
    "dist_high = y_train_high.sum(axis=0)/len(train_cleaned)\n",
    "\n",
    "print('Distribution of lower level labels:',dist_low)\n",
    "print('Distribution of higher level labels:',dist_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a function that will generate random predictions in the shapes that we need for the lower/higher level labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_predictions(input_len,label_dist):\n",
    "    \"\"\" Function that creates distribution-based random predictions for our higher/lower level labels.\n",
    "    Input:\n",
    "        input_len: The length of our dataset (i.e. how many predictions have to be generated).\n",
    "        label_dist: The distribution of labels (higher/lower level).\n",
    "    Output:\n",
    "        pred_collection: An array containing \"random\" predictions for every input paragraph.\n",
    "    \"\"\"\n",
    "    random.seed(11)\n",
    "    pred_collection = []\n",
    "    for i in range(input_len):\n",
    "        pred = []\n",
    "        for l in range(len(label_dist)):\n",
    "            x = random.random()\n",
    "            if x < label_dist[l]:\n",
    "                pred.append(1)\n",
    "            else:\n",
    "                pred.append(0)\n",
    "        pred_collection.append(pred)\n",
    "    return np.asarray(pred_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get \"random\" predictions for lower and higher level labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = len(test)\n",
    "\n",
    "pred_low = baseline_predictions(input_len,dist_low)\n",
    "pred_high = baseline_predictions(input_len,dist_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions for later comparison\n",
    "pklobj = open('data/pred_low_random.obj','wb')\n",
    "pickle.dump(pred_low,pklobj)\n",
    "pklobj.close()\n",
    "pklobj = open('data/pred_high_random.obj','wb')\n",
    "pickle.dump(pred_high,pklobj)\n",
    "pklobj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum lower level predictions:  [120  36  44  35  36  78   6]\n",
      "Sum higher level predictions:  [124  63 100]\n"
     ]
    }
   ],
   "source": [
    "import dontpatronizeme.ext_evaluation\n",
    "from importlib import reload\n",
    "reload(dontpatronizeme.ext_evaluation)\n",
    "\n",
    "print('Sum lower level predictions: ',pred_low.sum(axis=0))\n",
    "print('Sum higher level predictions: ',pred_high.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unbalanced Power Relations\n",
      "Accuracy: 0.632768361581921\n",
      "Precision: 0.825\n",
      "Recall: 0.6923076923076923\n",
      "F1 Score: 0.752851711026616\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[13 21]\n",
      " [44 99]]\n",
      "--------------------------------------------------\n",
      "Shallow Solution\n",
      "Accuracy: 0.6779661016949152\n",
      "Precision: 0.25\n",
      "Recall: 0.23076923076923078\n",
      "F1 Score: 0.24000000000000002\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[111  27]\n",
      " [ 30   9]]\n",
      "--------------------------------------------------\n",
      "Presupposition\n",
      "Accuracy: 0.6892655367231638\n",
      "Precision: 0.38636363636363635\n",
      "Recall: 0.37777777777777777\n",
      "F1 Score: 0.38202247191011235\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[105  27]\n",
      " [ 28  17]]\n",
      "--------------------------------------------------\n",
      "Authority Voice\n",
      "Accuracy: 0.632768361581921\n",
      "Precision: 0.22857142857142856\n",
      "Recall: 0.17391304347826086\n",
      "F1 Score: 0.19753086419753085\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[104  27]\n",
      " [ 38   8]]\n",
      "--------------------------------------------------\n",
      "Metaphor\n",
      "Accuracy: 0.7005649717514124\n",
      "Precision: 0.3055555555555556\n",
      "Recall: 0.28205128205128205\n",
      "F1 Score: 0.29333333333333333\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[113  25]\n",
      " [ 28  11]]\n",
      "--------------------------------------------------\n",
      "Compassion\n",
      "Accuracy: 0.4915254237288136\n",
      "Precision: 0.5256410256410257\n",
      "Recall: 0.43617021276595747\n",
      "F1 Score: 0.47674418604651164\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[46 37]\n",
      " [53 41]]\n",
      "--------------------------------------------------\n",
      "The poorer the merrier\n",
      "Accuracy: 0.9209039548022598\n",
      "Precision: 0.16666666666666666\n",
      "Recall: 0.1\n",
      "F1 Score: 0.125\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[162   5]\n",
      " [  9   1]]\n",
      "--------------------------------------------------\n",
      "F1 Score Average: 0.35249750950201486\n"
     ]
    }
   ],
   "source": [
    "dontpatronizeme.ext_evaluation.evaluate(y_test_low, pred_low, 'll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The saviour\n",
      "Accuracy: 0.6384180790960452\n",
      "Precision: 0.8467741935483871\n",
      "Recall: 0.7\n",
      "F1 Score: 0.7664233576642335\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[  8  19]\n",
      " [ 45 105]]\n",
      "--------------------------------------------------\n",
      "The expert\n",
      "Accuracy: 0.536723163841808\n",
      "Precision: 0.42857142857142855\n",
      "Recall: 0.3698630136986301\n",
      "F1 Score: 0.39705882352941174\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[68 36]\n",
      " [46 27]]\n",
      "--------------------------------------------------\n",
      "The poet\n",
      "Accuracy: 0.5480225988700564\n",
      "Precision: 0.63\n",
      "Recall: 0.5943396226415094\n",
      "F1 Score: 0.6116504854368932\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[34 37]\n",
      " [43 63]]\n",
      "--------------------------------------------------\n",
      "F1 Score Average: 0.5917108888768462\n"
     ]
    }
   ],
   "source": [
    "dontpatronizeme.ext_evaluation.evaluate(y_test_high, pred_high, 'hl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is somehow surprising to see how well our random baseline performs. It sets a higher standard for our actual models to compare to than we expected."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8f15eca31ea804bb0f6d33316ffa140c5a8921f35704d486c94dba9d6df5ff3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e150b9",
   "metadata": {},
   "source": [
    "# Multinomial Multi-Label Logistic Regression for PCL Detection\n",
    "\n",
    "Our first try will be to detect Patronizing and Condescending Language (PCL) using a Multi-Label Logistic Regression Approach. Logistic Regression is one simple approach for classification that was not previously explored in the paper, which is why we chose this as a first experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c47ae25",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "First we will load the dontpatronizeme-dataset from our previously saved train-/test-split, so that we have nicely stratified samples for learning and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "210ec765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dontpatronizeme.ext_dont_patronize_me import DontPatronizeMe\n",
    "\n",
    "# path to training set, path to test set\n",
    "dpm = DontPatronizeMe('./data/dpm_train.csv', './data/dpm_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fb2af55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>art_id</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "      <th>country</th>\n",
       "      <th>label</th>\n",
       "      <th>higher level label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1279</td>\n",
       "      <td>@@7896098</td>\n",
       "      <td>Pope Francis washed and kissed the feet of Mus...</td>\n",
       "      <td>refugee</td>\n",
       "      <td>ng</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4063</td>\n",
       "      <td>@@3002894</td>\n",
       "      <td>\"Budding chefs , like \"\" Fred \"\" , \"\" Winston ...</td>\n",
       "      <td>in-need</td>\n",
       "      <td>ie</td>\n",
       "      <td>[1, 0, 0, 1, 1, 1, 0]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4177</td>\n",
       "      <td>@@930041</td>\n",
       "      <td>The Word of God is truth that 's living and ab...</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>us</td>\n",
       "      <td>[1, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[1, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3963</td>\n",
       "      <td>@@18867357</td>\n",
       "      <td>Chantelle Owens , Mrs Planet 2016 , hosted the...</td>\n",
       "      <td>in-need</td>\n",
       "      <td>za</td>\n",
       "      <td>[1, 1, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[1, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2001</td>\n",
       "      <td>@@14012804</td>\n",
       "      <td>t is remiss not to mention here that not all s...</td>\n",
       "      <td>poor-families</td>\n",
       "      <td>tz</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   par_id      art_id                                               text  \\\n",
       "1    1279   @@7896098  Pope Francis washed and kissed the feet of Mus...   \n",
       "3    4063   @@3002894  \"Budding chefs , like \"\" Fred \"\" , \"\" Winston ...   \n",
       "6    4177    @@930041  The Word of God is truth that 's living and ab...   \n",
       "7    3963  @@18867357  Chantelle Owens , Mrs Planet 2016 , hosted the...   \n",
       "8    2001  @@14012804  t is remiss not to mention here that not all s...   \n",
       "\n",
       "         keyword country                  label higher level label  \n",
       "1        refugee      ng  [0, 1, 0, 0, 0, 0, 0]          [1, 0, 0]  \n",
       "3        in-need      ie  [1, 0, 0, 1, 1, 1, 0]          [1, 1, 1]  \n",
       "6       hopeless      us  [1, 0, 0, 0, 0, 1, 0]          [1, 0, 1]  \n",
       "7        in-need      za  [1, 1, 0, 0, 0, 1, 0]          [1, 0, 1]  \n",
       "8  poor-families      tz  [0, 0, 1, 0, 0, 0, 0]          [0, 1, 0]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpm.load_task2()\n",
    "dpm.train_task2_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ee228d",
   "metadata": {},
   "source": [
    "We can see that the data has been loaded correctly. Most important for training are the texts, which we will need to convert to embeddings next, as well as the normal labels and the higher level labels of the PCL taxonomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9854d0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>art_id</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "      <th>country</th>\n",
       "      <th>label</th>\n",
       "      <th>higher level label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4046</td>\n",
       "      <td>@@14767805</td>\n",
       "      <td>We also know that they can benefit by receivin...</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>us</td>\n",
       "      <td>[1, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8330</td>\n",
       "      <td>@@17252299</td>\n",
       "      <td>Many refugees do n't want to be resettled anyw...</td>\n",
       "      <td>refugee</td>\n",
       "      <td>ng</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4089</td>\n",
       "      <td>@@25597822</td>\n",
       "      <td>\"In a 90-degree view of his constituency , one...</td>\n",
       "      <td>homeless</td>\n",
       "      <td>pk</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>432</td>\n",
       "      <td>@@15802146</td>\n",
       "      <td>He depicts demonstrations by refugees at the b...</td>\n",
       "      <td>refugee</td>\n",
       "      <td>nz</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>369</td>\n",
       "      <td>@@15636898</td>\n",
       "      <td>\"\"\" People do n't understand the hurt , people...</td>\n",
       "      <td>women</td>\n",
       "      <td>ie</td>\n",
       "      <td>[1, 0, 1, 1, 0, 1, 0]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   par_id      art_id                                               text  \\\n",
       "0    4046  @@14767805  We also know that they can benefit by receivin...   \n",
       "2    8330  @@17252299  Many refugees do n't want to be resettled anyw...   \n",
       "4    4089  @@25597822  \"In a 90-degree view of his constituency , one...   \n",
       "5     432  @@15802146  He depicts demonstrations by refugees at the b...   \n",
       "9     369  @@15636898  \"\"\" People do n't understand the hurt , people...   \n",
       "\n",
       "    keyword country                  label higher level label  \n",
       "0  hopeless      us  [1, 0, 0, 1, 0, 0, 0]          [1, 1, 0]  \n",
       "2   refugee      ng  [0, 0, 1, 0, 0, 0, 0]          [0, 1, 0]  \n",
       "4  homeless      pk  [1, 0, 0, 0, 0, 0, 0]          [1, 0, 0]  \n",
       "5   refugee      nz  [0, 0, 0, 0, 0, 1, 0]          [0, 0, 1]  \n",
       "9     women      ie  [1, 0, 1, 1, 0, 1, 0]          [1, 1, 1]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpm.load_test()\n",
    "dpm.test_set_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c65a681",
   "metadata": {},
   "source": [
    "# Converting Paragraphs into Embeddings\n",
    "\n",
    "For training our logistic regression classifier, we need to convert the input sentences into sentence embeddings. More specifically, we need one vector per sentence that we can feed into our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a84d4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      Pope Francis washed and kissed the feet of Mus...\n",
       "3      \"Budding chefs , like \"\" Fred \"\" , \"\" Winston ...\n",
       "6      The Word of God is truth that 's living and ab...\n",
       "7      Chantelle Owens , Mrs Planet 2016 , hosted the...\n",
       "8      t is remiss not to mention here that not all s...\n",
       "                             ...                        \n",
       "987    Citing the fact that these kids who died at Go...\n",
       "988    Fern ? ndez was a well-known philanthropist wh...\n",
       "989    Touched much by their plight , Commanding Offi...\n",
       "990    She reiterated her ministry 's commitment to p...\n",
       "991    Preaching the sermon , the Dean of the St. Pet...\n",
       "Name: text, Length: 815, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs = dpm.train_task2_df.loc[:, 'text']\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7271023",
   "metadata": {},
   "source": [
    "Here we use scikit-learns TfidfVectorizer, which does this job nicely. We can also get our vocabulary from this Vectorizer for the training data, as we will need the same shape of vectors for the test data as well.\n",
    "\n",
    "Another approach would have been word2vec-like sentence embeddings or simply a bag-of-words-approach, but tf-idf already weighs the words according to their frequency, which might help in distinguishing content of texts better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aaea118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(815, 6859)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(paragraphs)\n",
    "print(X_train.shape)\n",
    "vocabulary = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b963b53",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Now for training, we need to extract the labels from the dataframe and train a classifier on them. For comparison, we start with a simple multiclass logistic regression, that is supposed to predict the entire set of labels as one class (as a string), to see if the multilabel approach has advantages here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e5b52e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast, numpy as np\n",
    "Y_train = dpm.train_task2_df.loc[:, 'label'].to_numpy()\n",
    "Y_train = np.array([np.array(ast.literal_eval(x)) for x in Y_train])\n",
    "Y_train[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2bb7b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(815, 6859)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5421ff6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4404907975460123"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normal logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, dpm.train_task2_df.loc[:, 'label'])\n",
    "classifier.score(X_train,dpm.train_task2_df.loc[:, 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8469c19",
   "metadata": {},
   "source": [
    "We can see that the normal (multiclass) logistic regression classifier performs okayish on the training data. Ideally a score should be better, of course, but as we will see in the next section the normal logistic regression performs even worse on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83e64c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3006134969325153"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multilabel logistic regression\n",
    "import numpy as np\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = MultiOutputClassifier(estimator= LogisticRegression(max_iter = 500)).fit(X_train, Y_train)\n",
    "clf.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf23bda3",
   "metadata": {},
   "source": [
    "Our multilabel logistic regression classifier is able to predict all 7 PCL categories independently of each other, thanks to the MultiOutputClassifier-Wrapper around the logistic regression. We can see that it performs worse on the train data than the normal logistic regression does, but we will explore the real results in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaa2bc3",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Let us try out both of our classifiers on the test set now. For that we need to convert the test paragraphs into the same embedding shape as our train paragraphs, which the TfidfVectorizer also does for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c252f232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(177, 6859)\n"
     ]
    }
   ],
   "source": [
    "test_paragraphs = dpm.test_set_df.loc[:, 'text']\n",
    "X_test = vectorizer.transform(test_paragraphs)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3aeaedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = dpm.test_set_df.loc[:, 'label'].to_numpy()\n",
    "Y_test = np.array([np.array(ast.literal_eval(x)) for x in Y_test])\n",
    "Y_test[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e51d4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14124293785310735"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normal logistic regression\n",
    "classifier.score(X_test,dpm.test_set_df.loc[:, 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "281e1ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1751412429378531"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multilabel logistic regression\n",
    "clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15016fb",
   "metadata": {},
   "source": [
    "We can observe that both scores drop significantly compared to our preliminary score results, but given the test data the multilabel classifier scores higher than the multiclass classifier. To see what our multilabel approach really does, we will now dive in into the evaluation of the predictions on every category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a3f50c",
   "metadata": {},
   "source": [
    "# Evaluating\n",
    "\n",
    "In order to properly evaluate the multilabel classification results we will use our extended evaluation script that will show us accuracy, precision, recall and F1 measure for each PCL category separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8da6fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([176,   0,   0,   1,   0,  61,   0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dontpatronizeme.ext_evaluation\n",
    "Y_pred = clf.predict(X_test)\n",
    "Y_pred.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea570bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving predictions for analysis\n",
    "import pickle\n",
    "pklobj = open('data/pred_logreg.obj','wb')\n",
    "pickle.dump(Y_pred, pklobj)\n",
    "pklobj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff325a97",
   "metadata": {},
   "source": [
    "It is striking to note that there are four categories which where all predicted to not appear anywhere in the data. A fifth category was only predicted to occur once. Let's see what our test script has to say to that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b73f842c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unbalanced Power Relations\n",
      "Accuracy: 0.8022598870056498\n",
      "Precision: 0.8068181818181818\n",
      "Recall: 0.993006993006993\n",
      "F1 Score: 0.890282131661442\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[  0  34]\n",
      " [  1 142]]\n",
      "--------------------------------------------------\n",
      "Shallow Solution\n",
      "Accuracy: 0.7796610169491526\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[138   0]\n",
      " [ 39   0]]\n",
      "--------------------------------------------------\n",
      "Presupposition\n",
      "Accuracy: 0.7457627118644068\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[132   0]\n",
      " [ 45   0]]\n",
      "--------------------------------------------------\n",
      "Authority Voice\n",
      "Accuracy: 0.7457627118644068\n",
      "Precision: 1.0\n",
      "Recall: 0.021739130434782608\n",
      "F1 Score: 0.042553191489361694\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[131   0]\n",
      " [ 45   1]]\n",
      "--------------------------------------------------\n",
      "Metaphor\n",
      "Accuracy: 0.7796610169491526\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[138   0]\n",
      " [ 39   0]]\n",
      "--------------------------------------------------\n",
      "Compassion\n",
      "Accuracy: 0.6666666666666666\n",
      "Precision: 0.7868852459016393\n",
      "Recall: 0.5106382978723404\n",
      "F1 Score: 0.6193548387096774\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[70 13]\n",
      " [46 48]]\n",
      "--------------------------------------------------\n",
      "The poorer the merrier\n",
      "Accuracy: 0.943502824858757\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[167   0]\n",
      " [ 10   0]]\n",
      "--------------------------------------------------\n",
      "F1 Score Average: 0.2217414516943544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "dontpatronizeme.ext_evaluation.evaluate(Y_test, Y_pred, 'll')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2df800",
   "metadata": {},
   "source": [
    "On the one hand, we can see that none of the categories Shallow Solution, Presupposition, Metaphor and The poorer the merrier have a single positive prediction. This also results in 0s for precision, recall and F1 score for these categories. Nonetheless, they all have a very high accuracy (all above 0.75) which suggests on the other hand that the model has learned that there are very few positive examples of this category to begin with and that it fares better if it categorically labels all of them as not containing this category. This can also be seen quite well in the mini confusion matrices for each category, which shows that most samples are true negatives and therefore naturally increase the accuracy.\n",
    "\n",
    "The exact opposite is happening to the category Unbalanced Power Relations. Here, the model predicts that every paragraph contains PCL of this category and scores very high with that generalisation in all four measures.\n",
    "\n",
    "For the category Authority Voice, the model seems to be very sure about one sample being positive (maybe linked to a single word that only exists in this paragraph and has been labeled in the training set with the same category). This way, we get a precision of 1, but the model is definitely lacking in recall and thus in its F1 measure.\n",
    "\n",
    "At last we have Compassion. This is the only category where the predictions are spread out across positive and negative samples and we can see that the model still seems to have learned to separate the occurences better than chance could have.\n",
    "\n",
    "So, even an approach as simple as logistic regression leads to acceptable results regarding the accuracy of the predictions, but it overgeneralises so much to fit the given dataset that it constantly predicts one often occurring category and never predicts four little occurring categories. With this in mind, all further results have to be handled with a lot of care."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa55c7f0",
   "metadata": {},
   "source": [
    "# Training on higher level labels\n",
    "\n",
    "Now onto something different. We have previously prepared the higher level labels of the taxonomy to match to the annotated lower level labels for each paragraph. We want to see whether predicting these labels is easier, as there are fewer labels to choose from while there are more samples to train on. We will use the already embedded sentences from the previous training, and only swap out the labels to train two new classifiers: one being again the \"normal\" logistic regression and the other being the multilabel logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bc90fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_hl = X_train\n",
    "Y_train_hl = dpm.train_task2_df.loc[:, 'higher level label'].to_numpy()\n",
    "Y_train_hl = np.array([np.array(ast.literal_eval(x)) for x in Y_train_hl])\n",
    "Y_train_hl[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "034773b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.750920245398773"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normal logistic regression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, dpm.train_task2_df.loc[:, 'higher level label'])\n",
    "classifier.score(X_train,dpm.train_task2_df.loc[:, 'higher level label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5db1143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6404907975460122"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multilabel logistic regression\n",
    "clf = MultiOutputClassifier(estimator= LogisticRegression(max_iter = 500)).fit(X_train_hl, Y_train_hl)\n",
    "clf.score(X_train_hl, Y_train_hl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a3c716",
   "metadata": {},
   "source": [
    "We can see that both score a lot higher on the train data itself, let's see the scores for our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5364634b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2598870056497175"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_hl = X_test\n",
    "classifier.score(X_test_hl, dpm.test_set_df.loc[:, 'higher level label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6704f522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 1],\n",
       "       [1, 0, 1],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_hl = dpm.test_set_df.loc[:, 'higher level label'].to_numpy()\n",
    "Y_test_hl = np.array([np.array(ast.literal_eval(x)) for x in Y_test_hl])\n",
    "Y_test_hl[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7fe5066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3954802259887006"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test_hl, Y_test_hl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b1d72",
   "metadata": {},
   "source": [
    "We can observe a similar pattern to the lower level predictions: the multilabel approach wins on the test data, although both models suffer drastic losses in terms of performance on the test set. Let's evaluate the categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27ff883a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([176,  24, 112])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_hl = clf.predict(X_test_hl)\n",
    "Y_pred_hl.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebea938",
   "metadata": {},
   "source": [
    "Predictions are spread out more evenly, although the 176 samples predicted to be of the first category are seemingly dominated by the lower level category Unbalanced Power Relations, as this predicted the same amount of paragraphs to contain this category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58213b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The saviour\n",
      "Accuracy: 0.8418079096045198\n",
      "Precision: 0.8465909090909091\n",
      "Recall: 0.9933333333333333\n",
      "F1 Score: 0.9141104294478527\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[  0  27]\n",
      " [  1 149]]\n",
      "--------------------------------------------------\n",
      "The expert\n",
      "Accuracy: 0.6666666666666666\n",
      "Precision: 0.7916666666666666\n",
      "Recall: 0.2602739726027397\n",
      "F1 Score: 0.39175257731958757\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[99  5]\n",
      " [54 19]]\n",
      "--------------------------------------------------\n",
      "The poet\n",
      "Accuracy: 0.7062146892655368\n",
      "Precision: 0.7410714285714286\n",
      "Recall: 0.7830188679245284\n",
      "F1 Score: 0.761467889908257\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[42 29]\n",
      " [23 83]]\n",
      "--------------------------------------------------\n",
      "F1 Score Average: 0.6891102988918991\n"
     ]
    }
   ],
   "source": [
    "dontpatronizeme.ext_evaluation.evaluate(Y_test_hl, Y_pred_hl, 'hl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c4c5f0",
   "metadata": {},
   "source": [
    "Compared to the lower level labels, the performance seems to improve across all these three categories. In the first category, \"The saviour\", the model is still labeling all paragraphs as containing this category, but the score improves because both lower level categories are now counting together for this category, so there are even fewer samples not containing this category at all.\n",
    "\n",
    "But, it seems that having Presupposition and Authority Voice together in \"The expert\" helps improving the scores a little bit. The prediction on the lower level included only one positive predicitons for Authority Voice, the rest was predicted negative. With our higher level prediction we can see that the model is more confident in classifying some samples as positive, so we do get a fairly good precision, although it still falls short on recall and thus F1 Score.\n",
    "\n",
    "In the third category, \"The poet\", we again have two lower level labels that were consistently predicted to never occur in the previous lower level classification, and we have the category of Compassion which was quite mixed but okay. Putting these three categories into one, we get improved measures overall, only the precision is dropping a little bit compared to having only compassion paragraphs.\n",
    "\n",
    "So, all in all, predicting the higher level labels seems to improve our model quite a bit, but it still does not result in perfection. But, if it helps predicting labels, that previously were constantly missing as for Presupposition and Authority Voice in \"The expert\", then this could mean that these two labels often go together and verify the taxonomy in this particular point. But also the overall higher scores speak for the taxonomy and against the size of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874582e5",
   "metadata": {},
   "source": [
    "# k-Fold Cross Validation\n",
    "\n",
    "At last, we want to see whether the Logistic Regression improves when training via a 10-fold cross validation, as has also been done by Pérez-Almendros et al. (2020). We will go back to the lower level labels with 7 categories for this experiment. Also, we will use the basic KFold implementation, as the StratifiedKFold cannot handle multilabel stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a02476a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1\n",
      "\t - Mean Train Accuracy: 30.4229%\n",
      "\t - Mean Validation Accuracy: 18.2927%\n",
      "\t - Mean F1 Score: 21.2969%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2\n",
      "\t - Mean Train Accuracy: 30.2865%\n",
      "\t - Mean Validation Accuracy: 21.9512%\n",
      "\t - Mean F1 Score: 22.106%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3\n",
      "\t - Mean Train Accuracy: 29.7408%\n",
      "\t - Mean Validation Accuracy: 23.1707%\n",
      "\t - Mean F1 Score: 23.0997%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4\n",
      "\t - Mean Train Accuracy: 29.4679%\n",
      "\t - Mean Validation Accuracy: 20.7317%\n",
      "\t - Mean F1 Score: 22.4392%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5\n",
      "\t - Mean Train Accuracy: 30.1501%\n",
      "\t - Mean Validation Accuracy: 20.7317%\n",
      "\t - Mean F1 Score: 20.6268%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6\n",
      "\t - Mean Train Accuracy: 29.7003%\n",
      "\t - Mean Validation Accuracy: 28.3951%\n",
      "\t - Mean F1 Score: 20.6946%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7\n",
      "\t - Mean Train Accuracy: 29.9728%\n",
      "\t - Mean Validation Accuracy: 20.9877%\n",
      "\t - Mean F1 Score: 20.4424%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8\n",
      "\t - Mean Train Accuracy: 30.109%\n",
      "\t - Mean Validation Accuracy: 18.5185%\n",
      "\t - Mean F1 Score: 21.5728%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9\n",
      "\t - Mean Train Accuracy: 29.7003%\n",
      "\t - Mean Validation Accuracy: 25.9259%\n",
      "\t - Mean F1 Score: 20.1412%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10\n",
      "\t - Mean Train Accuracy: 30.109%\n",
      "\t - Mean Validation Accuracy: 19.7531%\n",
      "\t - Mean F1 Score: 20.2604%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Mean Train Accuracy: 0.2997 (+- 0.0029)\n",
      "> Mean Validation Accuracy: 0.2185 (+- 0.0304)\n",
      "> Mean F1 Score: 0.2127 (+- 0.0096)\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state = 1)\n",
    "models = []\n",
    "train_scores = []\n",
    "validation_scores = []\n",
    "f1_scores = []\n",
    "fold_no = 1\n",
    "\n",
    "for train_index, test_index in kfold.split(X_train, Y_train):\n",
    "    # multilabel logistic regression\n",
    "    clf = MultiOutputClassifier(estimator= LogisticRegression(max_iter = 500)).fit(X_train[train_index], \n",
    "                                                                                   Y_train[train_index])\n",
    "    models.append(clf)\n",
    "    train_scores.append(clf.score(X_train[train_index], Y_train[train_index]))\n",
    "    validation_scores.append(clf.score(X_train[test_index], Y_train[test_index]))\n",
    "    f1_scores.append(dontpatronizeme.ext_evaluation.evaluate(Y_train[test_index], \n",
    "                                                             clf.predict(X_train[test_index]), \n",
    "                                                             'll', verbose = False))\n",
    "    fold_no += 1\n",
    "       \n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(train_scores)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1}')\n",
    "    print(f'\\t - Mean Train Accuracy: {round(train_scores[i] *100, 4) }%')\n",
    "    print(f'\\t - Mean Validation Accuracy: {round(validation_scores[i] *100, 4)}%')\n",
    "    print(f'\\t - Mean F1 Score: {round(f1_scores[i] * 100, 4)}%')\n",
    "\n",
    "\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Mean Train Accuracy: {round(np.mean(train_scores), 4)} (+- {round(np.std(train_scores), 4)})')\n",
    "print(f'> Mean Validation Accuracy: {round(np.mean(validation_scores), 4)} (+- {round(np.std(validation_scores), 4)})')\n",
    "print(f'> Mean F1 Score: {round(np.mean(f1_scores), 4)} (+- {round(np.std(f1_scores), 4)})')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9840753",
   "metadata": {},
   "source": [
    "Seeing that the model from Fold 3 has the best F1 Score with 23%, we will now test this model with our separate test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a21cb2cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unbalanced Power Relations\n",
      "Accuracy: 0.8022598870056498\n",
      "Precision: 0.8068181818181818\n",
      "Recall: 0.993006993006993\n",
      "F1 Score: 0.890282131661442\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[  0  34]\n",
      " [  1 142]]\n",
      "--------------------------------------------------\n",
      "Shallow Solution\n",
      "Accuracy: 0.7796610169491526\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[138   0]\n",
      " [ 39   0]]\n",
      "--------------------------------------------------\n",
      "Presupposition\n",
      "Accuracy: 0.7457627118644068\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[132   0]\n",
      " [ 45   0]]\n",
      "--------------------------------------------------\n",
      "Authority Voice\n",
      "Accuracy: 0.7457627118644068\n",
      "Precision: 1.0\n",
      "Recall: 0.021739130434782608\n",
      "F1 Score: 0.042553191489361694\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[131   0]\n",
      " [ 45   1]]\n",
      "--------------------------------------------------\n",
      "Metaphor\n",
      "Accuracy: 0.7796610169491526\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[138   0]\n",
      " [ 39   0]]\n",
      "--------------------------------------------------\n",
      "Compassion\n",
      "Accuracy: 0.655367231638418\n",
      "Precision: 0.8\n",
      "Recall: 0.46808510638297873\n",
      "F1 Score: 0.5906040268456376\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[72 11]\n",
      " [50 44]]\n",
      "--------------------------------------------------\n",
      "The poorer the merrier\n",
      "Accuracy: 0.943502824858757\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix: (tn, fp / fn, tp)\n",
      "[[167   0]\n",
      " [ 10   0]]\n",
      "--------------------------------------------------\n",
      "F1 Score Average: 0.2176341928566345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "Y_pred = models[3-1].predict(X_test)\n",
    "dontpatronizeme.ext_evaluation.evaluate(Y_test, Y_pred, 'll')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1916ef02",
   "metadata": {},
   "source": [
    "Here, we get an average F1 Score over all 7 categories of 0.21. This is slightly lower than the regular 0.22 average from the plain Logistic Regression. We also still get similar phenomena with cross validation, including always or never predicting certain categories to occur. This leads to the conclusion, that also the 10-fold cross validation did not have a significant impact on the training or selection of the Logistic Regression models, as the results are quite similar and did not improve much."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
